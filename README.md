# 使用vLLM本地部署Qwen模型
使用方法
1、下载qwen模型文件，放到model文件夹下
2、运行start_vllm.sh
3、运行 chat.py
```
python chat.py
```
